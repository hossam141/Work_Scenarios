# ğŸ“Œ Fixing EKS Pods Stuck in Pending Due to Insufficient Worker Nodes

## **ğŸ”¹ Scenario**
A **Site Reliability Engineer (SRE)** notices that new application pods are **stuck in a `Pending` state** in an AWS EKS cluster.

---

## **ğŸ”¹ Step 1: Identify the Issue**

### **1ï¸âƒ£ Check Pod Status**
```bash
kubectl get pods -A
```
ğŸ”¹ **Example Output:**
```
NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE
default       web-app-57cdb987cd-xyz12      0/1     Pending   0          10m
```
âœ… **Pods are stuck in `Pending` for more than 10 minutes.**

---

### **2ï¸âƒ£ Check Pod Events for More Details**
```bash
kubectl describe pod web-app-57cdb987cd-xyz12
```
ğŸ”¹ **Example Output:**
```
Events:
  Type     Reason              Age                From               Message
  ----     ------              ----               ----               -------
  Warning  FailedScheduling    10m                default-scheduler  0/3 nodes are available: insufficient cpu, insufficient memory.
```
âœ… **The error message shows that there are not enough compute resources (CPU, Memory) available.**

---

## **ğŸ”¹ Step 2: Check Worker Node Capacity**

### **1ï¸âƒ£ List All Nodes**
```bash
kubectl get nodes
```
ğŸ”¹ **Example Output:**
```
NAME                                      STATUS   ROLES    AGE    VERSION
ip-192-168-10-100.ec2.internal   Ready    <none>   10d   v1.26.2
ip-192-168-10-101.ec2.internal   Ready    <none>   10d   v1.26.2
ip-192-168-10-102.ec2.internal   Ready    <none>   10d   v1.26.2
```
âœ… **Nodes are healthy but are at full capacity.**

---

## **ğŸ”¹ Step 3: Verify Auto Scaling Configuration**

### **1ï¸âƒ£ Check Cluster Autoscaler Logs**
```bash
kubectl logs -f deployment/cluster-autoscaler -n kube-system
```
ğŸ”¹ **Example Output:**
```
No upcoming scale-up event detected.
Cluster is at maximum node capacity.
```
âœ… **Autoscaler is not increasing nodes because the maximum capacity is reached.**

---

## **ğŸ”¹ Step 4: Increase ASG Capacity for Worker Nodes**

### **1ï¸âƒ£ Modify the Auto Scaling Group (ASG) to Allow Scaling**
```bash
aws autoscaling update-auto-scaling-group \
  --auto-scaling-group-name my-eks-node-group \
  --min-size 3 \
  --desired-capacity 5 \
  --max-size 10
```
âœ… **Now, the ASG will launch more worker nodes when needed.**

---

## **ğŸ”¹ Step 5: Verify New Nodes Are Added**

### **1ï¸âƒ£ Check Available Nodes Again**
```bash
kubectl get nodes
```
ğŸ”¹ **Example Output:**
```
NAME                                      STATUS   ROLES    AGE    VERSION
ip-192-168-10-100.ec2.internal   Ready    <none>   10d   v1.26.2
ip-192-168-10-101.ec2.internal   Ready    <none>   10d   v1.26.2
ip-192-168-10-102.ec2.internal   Ready    <none>   10d   v1.26.2
ip-192-168-10-103.ec2.internal   Ready    <none>   1m    v1.26.2
ip-192-168-10-104.ec2.internal   Ready    <none>   1m    v1.26.2
```
âœ… **New nodes are now available!**

---

## **ğŸ”¹ Step 6: Verify Pods Are Running**

### **1ï¸âƒ£ Check Pods Again**
```bash
kubectl get pods -A
```
ğŸ”¹ **Example Output:**
```
NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE
default       web-app-57cdb987cd-xyz12      1/1     Running   0          2m
```
âœ… **Pods are now running successfully on new worker nodes! ğŸš€**

---

## **ğŸ“Œ Summary of the Issue & Fix**

| **Step** | **Command Used** | **Outcome** |
|----------|----------------|-------------|
| **Check Pod Status** | `kubectl get pods -A` | Identified that pods were in `Pending`. |
| **Check Scheduling Issues** | `kubectl describe pod <pod-name>` | Found `insufficient cpu, insufficient memory`. |
| **Check Available Nodes** | `kubectl get nodes` | Nodes were at full capacity. |
| **Check Autoscaler Logs** | `kubectl logs -f deployment/cluster-autoscaler -n kube-system` | No upcoming scale-up event detected. |
| **Increase ASG Capacity** | `aws autoscaling update-auto-scaling-group` | Allowed new worker nodes to be added. |
| **Verify New Nodes** | `kubectl get nodes` | New nodes successfully added. |
| **Verify Pods Running** | `kubectl get pods -A` | Pods moved from `Pending` to `Running`. |

ğŸš€ **Now the cluster is scaling properly, and the application is running smoothly!**
